{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as T, functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"dl.cfg\")\n",
    "\n",
    "KEY = config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "SECRET = config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get('AWS',\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config.get('AWS',\"AWS_SECRET_ACCESS_KEY\")\n",
    "os.environ['HADOOP_HOME'] = \"C:/winutils-master/winutils-master/hadoop-2.7.1/bin/\"\n",
    "#System.setProperty(\"hadoop.home.dir\", \"C:\\Hadoop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.1\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_state(x):\n",
    "    return x.strip().split('-')[-1]\n",
    "udf_parse_state = F.udf(lambda x: parse_state(x), T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = spark.read.format('csv').load(\"s3a://capstonekndend/us-cities-demographics.csv\", header = True, inferSchema = True, sep = ';') \\\n",
    "                 .select(\"State Code\", \"City\") \\\n",
    "                 .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "                 .withColumnRenamed(\"City\", \"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_airport = spark.read.format('csv').load(\"s3a://capstonekndend/airport_codes.csv\", header = True, inferSchema = True) \\\n",
    "                       .filter(\"iso_country == 'US' \") \\\n",
    "                       .withColumn(\"state\", udf_parse_state(\"iso_region\")) \\\n",
    "                       .selectExpr(\"state AS state_code\", \"municipality AS city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = us_airport.union(demo) \\\n",
    "                 .drop_duplicates() \\\n",
    "                 .withColumn(\"city_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "city.write.mode(\"overwrite\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/city/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lat(x):\n",
    "    y = x.strip().split(',')\n",
    "    return float(y[0])\n",
    "udf_parse_lat = F.udf(lambda x: parse_lat(x), T.FloatType())\n",
    "\n",
    "def parse_long(x):\n",
    "    y = x.strip().split(',')\n",
    "    return float(y[1])\n",
    "udf_parse_long = F.udf(lambda x: parse_long(x), T.FloatType())\n",
    "\n",
    "def parse_state(x):\n",
    "    return x.strip().split('-')[-1]\n",
    "udf_parse_state = F.udf(lambda x: parse_state(x), T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/chait/Desktop/Udacity/I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')\n",
    "\n",
    "def code_mapper(file, idx):\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic\n",
    "\n",
    "i94cit_res = code_mapper(f_content, \"i94cntyl\")\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94visa = {'1':'Business', '2':'Pleasure', '3':'Student'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state = list(map(list, i94addr.items()))\n",
    "df_state= spark.createDataFrame(df_state, [\"state_code\", \"state\"])\n",
    "\n",
    "df_state.write.mode(\"overwrite\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/codes/state_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = list(map(list, i94cit_res.items()))\n",
    "df_country = spark.createDataFrame(df_country, [\"country_code\", \"country\"])\n",
    "\n",
    "df_country.write.mode(\"overwrite\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/codes/country_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_airport = spark.read.format('csv').load(\"s3a://capstonekndend/airport_codes.csv\", header = True, inferSchema = True) \\\n",
    "                      .withColumn(\"airport_latitude\", udf_parse_lat(\"coordinates\")) \\\n",
    "                      .withColumn(\"airport_longitude\", udf_parse_long(\"coordinates\")) \\\n",
    "                      .filter(\"iso_country == 'US' \") \\\n",
    "                      .withColumn(\"state\", udf_parse_state(\"iso_region\")) \\\n",
    "                      .withColumnRenamed(\"ident\", \"icao_code\") \\\n",
    "                      .drop(\"coordinates\", \"gps_code\", \"local_code\", \"continent\", \"iso_region\", \"iso_country\", \"iata_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_airport = us_airport.join(city, (us_airport.municipality == city.city) & (us_airport.state == city.state_code), \"left\").drop(\"municipality\", \"state\", \"city\", \"state_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_airport.write.mode(\"overwrite\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/codes/airport_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = spark.read.format('csv').load('s3a://capstonekndend/us-cities-demographics.csv', header = True, inferSchema = True, delimiter = ';') \\\n",
    "                 .withColumn(\"male_population\", col(\"Male Population\").cast(T.LongType())) \\\n",
    "                 .withColumn(\"female_population\", col(\"Female Population\").cast(T.LongType())) \\\n",
    "                 .withColumn(\"total_population\", col(\"Total Population\").cast(T.LongType())) \\\n",
    "                 .withColumn(\"num_veterans\", col(\"Number of Veterans\").cast(T.LongType())) \\\n",
    "                 .withColumn(\"foreign_born\", col(\"Foreign-born\").cast(T.LongType())) \\\n",
    "                 .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "                 .withColumnRenamed(\"Race\", \"race\") \\\n",
    "                 .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "                 .withColumnRenamed(\"City\", \"city\") \\\n",
    "                 .withColumnRenamed(\"Average Household Size\", \"average_household_size\") \\\n",
    "                 .drop(\"State\", \"Count\", \"Male Population\", \"Female Population\", \"Total Population\", \"Number of Veterans\", \"Foreign-born\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = demo.join(city, (demo.city == city.city) & (demo.state_code == city.state_code)).drop(\"city\", \"state_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.write.mode(\"overwrite\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/us_cities_demographics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_latitude(x):\n",
    "    direction = str(x)[-1]\n",
    "    if direction == 'N':\n",
    "        return float(str(x))[:-1]\n",
    "    else:\n",
    "        return -float(str(x))[:-1]\n",
    "udf_convert_latitude = F.udf(lambda x: convert_latitude(x), T.FloatType())\n",
    "\n",
    "\n",
    "def convert_longitude(x):\n",
    "    direction = str(x)[-1]\n",
    "    if direction == 'E':\n",
    "        return float(str(x))[:-1]\n",
    "    else:\n",
    "        return -float(str(x))[:-1]\n",
    "udf_convert_longitude = F.udf(lambda x: convert_longitude(x), T.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = F.to_date(F.lit(\"2013-08-01\")).cast(T.TimestampType())\n",
    "us_weather = spark.read.format('csv').load(\"s3a://capstonekndend/GlobalLandTemperaturesByCity.csv\", header = True, inferSchema = True) \\\n",
    "                       .where(col(\"dt\") > thres) \\\n",
    "                       .withColumn(\"latitude\", udf_convert_latitude(\"Latitude\")) \\\n",
    "                       .withColumn(\"longitude\", udf_convert_longitude(\"Longitude\")) \\\n",
    "                       .withColumnRenamed(\"AverageTemperature\", \"avg_temp\") \\\n",
    "                       .withColumnRenamed(\"AverageTemperatureUncertainty\", \"std_temp\") \\\n",
    "                       .withColumnRenamed(\"City\", \"city\") \\\n",
    "                       .withColumnRenamed(\"Country\", \"country\") \\\n",
    "                       .where(col(\"avg_temp\").isNotNull()) \\\n",
    "                       .filter(\"country = 'United States' \") \\\n",
    "                       .drop(\"dt\", \"country\", \"latitude\", \"Longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_weather = us_weather.join(city, \"city\", \"left\").drop(\"city\", \"state_code\", \"city_latitude\", \"city_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_weather.write.mode(\"overwrite\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/us_cities_temperatures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_weather = us_airport.select(\"name\", \"elevation_ft\", \"city_id\") \\\n",
    "                            .join(city.select(\"city\", \"city_id\", \"state_code\"), \"city_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_weather = airport_weather.join(df_state, (airport_weather.state_code == df_state.state_code), \"left\").drop(\"state_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_weather = airport_weather.join(us_weather, \"city_id\", \"inner\").drop(\"city_id\").drop_duplicates(subset = ['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_weather.write.mode(\"overwrite\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/us_airports_weather/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days = int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_to_datetime_sas = F.udf(lambda x: to_datetime(x), T.DateType())\n",
    "\n",
    "\n",
    "\n",
    "def to_datetimefrstr(x):\n",
    "    try:\n",
    "        return datetime.strptime(x, '%m%d%Y')\n",
    "    except:\n",
    "        return None\n",
    "udf_to_datetimefrstr = F.udf(lambda x: to_datetimefrstr(x), T.DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrant = spark.read.parquet(\"s3a://capstonekndend/sas_data/\") \\\n",
    "                      .selectExpr('cast(cicid as int) AS cicid', 'arrdate', 'cast(i94res as int) AS from_country_code', 'cast(i94bir as int) AS age',\n",
    "                                  'cast(i94visa as int) AS visa_code', 'visapost AS visa_post', 'occup AS occupation',\n",
    "                                  'visatype AS visa_type', 'cast(biryear as int) AS birth_year', 'gender') \\\n",
    "                      .withColumn(\"arrival_date\", udf_to_datetime_sas(\"arrdate\")) \\\n",
    "                      .withColumn(\"arrival_month\", F.month(\"arrival_date\")) \\\n",
    "                      .withColumn(\"arrival_year\", F.year(\"arrival_date\")) \\\n",
    "                      .withColumn(\"month\", F.month(\"arrival_date\")) \\\n",
    "                      .withColumn(\"year\", F.year(\"arrival_date\")) \\\n",
    "                      .drop(\"arrdate\")\n",
    "\n",
    "immigrant.write.mode(\"append\").partitionBy(\"year\", \"month\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/immigrant/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration = spark.read.parquet(\"s3a://capstonekndend/sas_data/\") \\\n",
    "                      .selectExpr('cast(cicid as int) cicid', 'arrdate', 'i94port AS iata_code', 'i94addr AS state_code', 'depdate', 'dtaddto', 'airline', 'cast(admnum as long) admnum', 'fltno', 'entdepa', 'entdepd', 'entdepu', 'matflag') \\\n",
    "                      .withColumnRenamed(\"entdepa\", \"arrival_flag\") \\\n",
    "                      .withColumnRenamed(\"entdepd\", \"departure_flag\") \\\n",
    "                      .withColumnRenamed(\"entdepu\", \"update_flag\") \\\n",
    "                      .withColumn(\"arrival_date\", udf_to_datetime_sas(\"arrdate\")) \\\n",
    "                      .withColumn(\"departure_date\", udf_to_datetime_sas(\"depdate\")) \\\n",
    "                      .withColumn(\"allowed_until\", udf_to_datetimefrstr(\"dtaddto\")) \\\n",
    "                      .withColumn(\"arrival_month\", F.month(\"arrival_date\")) \\\n",
    "                      .withColumn(\"arrival_year\", F.year(\"arrival_date\")) \\\n",
    "                      .withColumn(\"month\", F.month(\"arrival_date\")) \\\n",
    "                      .withColumn(\"year\", F.year(\"arrival_date\")) \\\n",
    "                      .drop(\"arrdate\", \"depdate\", \"dtaddto\", \"matflag\")\n",
    "\n",
    "immigration.write.mode(\"append\").partitionBy(\"year\", \"month\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/immigration/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = spark.read.options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/us_cities_demographics/\") \\\n",
    "                 .select(\"median_age\", \"city_id\", \"total_population\", \"foreign_born\") \\\n",
    "                 .join(city.select(\"state_code\", \"city_id\"), \"city_id\") \\\n",
    "                 .drop(\"city_id\") \\\n",
    "                 .groupBy(\"state_code\").agg(F.mean(\"median_age\").alias(\"median_age\"),\n",
    "                                            F.sum(\"total_population\").alias(\"total_population\"),\n",
    "                                            F.sum(\"foreign_born\").alias(\"foreign_born\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_demographic = immigrant.select(\"cicid\", \"from_country_code\", \"age\", \"gender\", \"occupation\", \"arrival_date\", \"arrival_month\", \"arrival_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_demographic = immigration_demographic.join(df_country, (immigration_demographic.from_country_code == df_country.country_code), \"left\") \\\n",
    "                                                 .withColumn(\"month\", F.month(\"arrival_date\")) \\\n",
    "                                                 .withColumn(\"year\", F.year(\"arrival_date\")) \\\n",
    "                                                 .withColumnRenamed(\"country\", \"from_country\") \\\n",
    "                                                 .drop(\"from_country_code\", \"country_code\", \"arrival_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_demographic = immigration_demographic.join(immigration.select(\"cicid\", \"state_code\"), \"cicid\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_demographic = immigration_demographic.join(df_state, (immigration_demographic.state_code == df_state.state_code),  \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_demographic = immigration_demographic.join(demo, \"state_code\").drop(\"state_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_demographic.write.partitionBy(\"year\", \"month\").mode(\"append\").options(header = True, inferSchema = True).csv(\"s3a://capstonekndend/csv/lake/immigration_demographic/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
